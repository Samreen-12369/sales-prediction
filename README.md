# Sales Prediction - Regression Model

A comprehensive machine learning project for predicting future product sales using multiple regression algorithms with time-series features and production-ready deployment strategies.

## Overview

This Jupyter notebook implements an end-to-end sales forecasting system that helps businesses predict future revenue, optimize inventory planning, and support strategic decision-making. The project trains and evaluates three different regression models, incorporates time-based features, and provides detailed deployment recommendations.

## Table of Contents

- [Business Value](#business-value)
- [Key Features](#key-features)
- [Technical Stack](#technical-stack)
- [Dataset Description](#dataset-description)
- [Model Architecture](#model-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Model Performance](#model-performance)
- [Feature Importance](#feature-importance)
- [Deployment Guide](#deployment-guide)
- [Monitoring & Maintenance](#monitoring--maintenance)
- [Best Practices](#best-practices)

## Business Value

### Primary Benefits

1. **Accurate Sales Forecasting**: Predict future revenue with high confidence
2. **Inventory Optimization**: Reduce waste and stockouts through demand prediction
3. **Resource Allocation**: Optimize marketing spend and sales team deployment
4. **Strategic Planning**: Support data-driven budgeting and goal setting
5. **Risk Mitigation**: Identify potential shortfalls early

### Use Cases

- **Inventory Management**: Forecast product demand by category and region
- **Financial Planning**: Generate weekly/monthly/quarterly revenue projections
- **Sales Strategy**: Identify high-potential opportunities and customer segments
- **Marketing Optimization**: Target campaigns based on predicted performance
- **Supply Chain**: Improve procurement and distribution planning

## Key Features

### ðŸŽ¯ Multiple Regression Models

1. **Linear Regression**
   - Baseline interpretable model
   - Fast training and inference
   - Good for understanding linear relationships
   - Coefficients reveal feature importance

2. **Random Forest Regressor**
   - Ensemble of decision trees
   - Handles non-linear patterns
   - Built-in feature importance
   - Resistant to overfitting
   - Excellent generalization

3. **XGBoost (Extreme Gradient Boosting)**
   - State-of-the-art gradient boosting
   - Superior predictive accuracy
   - Handles complex interactions
   - Efficient parallel processing

### ðŸ“Š Comprehensive Evaluation

- **RMSE (Root Mean Squared Error)**: Measures prediction accuracy
- **MAE (Mean Absolute Error)**: Average absolute prediction error
- **RÂ² Score**: Proportion of variance explained
- **Cross-Validation**: 5-fold CV for robust performance estimates
- **Residual Analysis**: Systematic error detection
- **Time Series Split**: Proper temporal validation

### ðŸ”§ Advanced Feature Engineering

- **Temporal Features**: Year, month, quarter, day of week
- **Cyclical Encoding**: Capture seasonality patterns
- **Categorical Encoding**: Label encoding for categorical variables
- **Feature Scaling**: StandardScaler for numerical stability
- **Interaction Terms**: Capture feature relationships

### ðŸ“ˆ Visualization Suite

- Model performance comparison charts
- Actual vs predicted scatter plots
- Residual distribution analysis
- Feature importance rankings
- Time-series prediction plots
- Error analysis visualizations

### ðŸš€ Production-Ready

- Model serialization (pickle format)
- Preprocessor artifacts saved
- Deployment recommendations
- Monitoring strategy
- API integration examples

## Technical Stack

### Core Libraries

- **pandas**: Data manipulation and analysis
- **numpy**: Numerical computing
- **matplotlib**: Static visualizations
- **seaborn**: Statistical data visualization
- **scikit-learn**: Machine learning framework
- **XGBoost**: Gradient boosting library

### ML Components

```python
# Model Training
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# Preprocessing
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Evaluation
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
```

## Dataset Description

### Required Features

| Feature | Description | Type | Example |
|---------|-------------|------|---------|
| **Order_ID** | Unique order identifier | Integer | 4971 |
| **Order_Date** | Transaction date | Date | 2021-01-01 |
| **Region** | Geographic region | Categorical | North, South, East, West |
| **Country** | Country name | Categorical | USA, Brazil, China, Canada |
| **Sales_Rep** | Sales representative name | Categorical | David Lee |
| **Team** | Sales team assignment | Categorical | Team A, B, C |
| **Customer_ID** | Unique customer identifier | String | C3971 |
| **Customer_Segment** | Customer category | Categorical | Corporate, SME, Retail |
| **Product_Category** | Product type | Categorical | Furniture, Electronics, Appliances |
| **Product_Name** | Specific product | Categorical | Office Chair, Laptop Pro |
| **Stage** | Deal stage | Categorical | Won, Lost, Opportunity |
| **Units_Sold** | Quantity sold | Integer | 6 |
| **Revenue** | Sales revenue (target) | Float | 1662.00 |
| **Target** | Sales target | Float | 1447.00 |
| **Deal_Size** | Deal value | Float | 1665.00 |

### Dataset Statistics

- **Total Records**: 5,000 orders
- **Features**: 15 columns (11 input features + 4 metadata)
- **Target Variable**: Revenue (continuous)
- **Date Range**: Full year of historical data
- **No Missing Values**: Clean dataset

### Data Format

```csv
Order_ID,Order_Date,Region,Country,Sales_Rep,Team,Customer_ID,Customer_Segment,Product_Category,Product_Name,Stage,Units_Sold,Revenue,Target,Deal_Size
4971,2021-01-01,North,USA,David Lee,Team C,C3971,Corporate,Furniture,Office Chair,Won,6,1662,1447,1665
```

## Model Architecture

### 1. Data Preprocessing Pipeline

#### Step 1: Date Feature Engineering

```python
# Extract temporal features
df['Year'] = df['Order_Date'].dt.year
df['Month'] = df['Order_Date'].dt.month
df['Quarter'] = df['Order_Date'].dt.quarter
df['Day_of_Week'] = df['Order_Date'].dt.dayofweek
df['Week_of_Year'] = df['Order_Date'].dt.isocalendar().week
```

#### Step 2: Categorical Encoding

```python
# Label encoding for categorical variables
categorical_features = ['Region', 'Country', 'Sales_Rep', 'Team', 
                       'Customer_Segment', 'Product_Category', 'Product_Name', 'Stage']

label_encoders = {}
for col in categorical_features:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
```

#### Step 3: Feature Scaling

```python
# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

#### Step 4: Train-Test Split

```python
# 80-20 temporal split (respecting time order)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=False
)
```

### 2. Model Training

#### Linear Regression

```python
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Pros: Fast, interpretable, low computational cost
# Cons: Assumes linear relationships, may underfit complex data
```

#### Random Forest

```python
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=20,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)

# Pros: Handles non-linearity, robust, feature importance
# Cons: Slower than linear models, less interpretable
```

#### XGBoost

```python
xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb_model.fit(X_train, y_train)

# Pros: Best accuracy, handles missing data, regularization
# Cons: Longer training time, requires tuning
```

### 3. Model Evaluation

#### Metrics Computed

```python
# For each model
predictions = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Cross-validation
cv_scores = cross_val_score(model, X_train, y_train, 
                           cv=5, scoring='neg_mean_squared_error')
cv_rmse = np.sqrt(-cv_scores.mean())
```

#### Time Series Validation

```python
# Proper time-based cross-validation
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, val_idx in tscv.split(X):
    # Train and validate on sequential folds
    pass
```

## Installation

### Prerequisites

- Python 3.7+
- Jupyter Notebook or JupyterLab
- pip package manager

### Step-by-Step Setup

1. **Create Virtual Environment**

```bash
python -m venv sales_pred_env
source sales_pred_env/bin/activate  # Windows: sales_pred_env\Scripts\activate
```

2. **Install Dependencies**

```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost
```

3. **Launch Jupyter**

```bash
jupyter notebook sales_prediction.ipynb
```

### Required Packages

```txt
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
xgboost>=1.5.0
```

## Usage

### Running the Notebook

1. **Prepare Your Data**
   - Ensure CSV file matches the required format
   - Place file in the same directory
   - Update file path in the notebook:
     ```python
     df = pd.read_csv('your_sales_data.csv')
     ```

2. **Execute Sequentially**
   - Run all cells from top to bottom
   - Review outputs and visualizations
   - Inspect model performance metrics

3. **Notebook Sections**
   - **Section 1**: Import libraries and load data
   - **Section 2**: Exploratory data analysis
   - **Section 3**: Feature engineering
   - **Section 4**: Data preprocessing
   - **Section 5**: Model training (3 models)
   - **Section 6**: Model evaluation and comparison
   - **Section 7**: Feature importance analysis
   - **Section 8**: Residual analysis
   - **Section 9**: Model serialization
   - **Section 10**: Deployment recommendations

### Customization Options

#### Adjust Model Hyperparameters

```python
# Random Forest
rf_model = RandomForestRegressor(
    n_estimators=200,        # More trees
    max_depth=30,            # Deeper trees
    min_samples_split=10,    # Prevent overfitting
    random_state=42
)

# XGBoost
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    learning_rate=0.05,      # Slower learning
    max_depth=8,             # More complex trees
    subsample=0.7,           # Row sampling
    colsample_bytree=0.7,    # Column sampling
    gamma=1,                 # Regularization
    random_state=42
)
```

#### Change Train-Test Split

```python
# 70-30 split instead of 80-20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, shuffle=False
)
```

#### Add Custom Features

```python
# Create interaction features
df['Revenue_per_Unit'] = df['Revenue'] / (df['Units_Sold'] + 1)
df['Target_Achievement'] = df['Revenue'] / df['Target']
df['Price_Point'] = df['Deal_Size'] / (df['Units_Sold'] + 1)
```

## Model Performance

### Expected Results

Based on typical performance with similar datasets:

| Model | RMSE | MAE | RÂ² Score | CV RMSE |
|-------|------|-----|----------|---------|
| **Linear Regression** | $180 - $220 | $140 - $170 | 0.85 - 0.90 | $190 - $230 |
| **Random Forest** | $120 - $160 | $90 - $120 | 0.92 - 0.96 | $130 - $170 |
| **XGBoost** | $100 - $140 | $75 - $100 | 0.94 - 0.97 | $110 - $150 |

### Model Selection

**Best Model: XGBoost** (typically)

**Reasons:**
1. Lowest RMSE and MAE
2. Highest RÂ² score
3. Best cross-validation performance
4. Handles non-linear relationships
5. Built-in regularization prevents overfitting

**When to Use Alternatives:**
- **Linear Regression**: Simple deployment, interpretability needed
- **Random Forest**: Balance of accuracy and speed, feature importance

### Interpretation

- **RMSE**: Average prediction error in dollars
  - Lower is better
  - Sensitive to large errors
  
- **MAE**: Average absolute prediction error
  - More robust to outliers
  - Easier to interpret
  
- **RÂ² Score**: Percentage of variance explained
  - 1.0 = perfect predictions
  - 0.0 = baseline (mean) predictions
  - 0.95 = model explains 95% of variance

## Feature Importance

### Top Predictive Features

Based on Random Forest and XGBoost analysis:

1. **Deal_Size** (35-40%)
   - Strongest predictor of revenue
   - Direct relationship with sales value

2. **Target** (20-25%)
   - Sales target influences outcomes
   - Indicates deal potential

3. **Units_Sold** (15-20%)
   - Quantity directly impacts revenue
   - Key volume indicator

4. **Product_Category** (10-15%)
   - Different categories have different revenue patterns
   - Electronics typically higher value

5. **Customer_Segment** (8-12%)
   - Corporate clients tend to larger deals
   - Retail smaller but more frequent

6. **Region/Country** (5-10%)
   - Geographic differences in purchasing power
   - Market maturity varies

7. **Temporal Features** (5-8%)
   - Monthly seasonality
   - Quarterly trends

8. **Sales_Rep/Team** (3-5%)
   - Performance variation
   - Experience and skill differences

### Business Insights

- Focus on high-value product categories
- Prioritize corporate segment for large deals
- Align targets with historical performance
- Consider seasonal patterns in forecasting
- Invest in underperforming regions

## Deployment Guide

### Production Inference

```python
import pickle
import pandas as pd
import numpy as np

# Load saved artifacts
model = pickle.load(open('sales_model.pkl', 'rb'))
scaler = pickle.load(open('scaler.pkl', 'rb'))
label_encoders = pickle.load(open('label_encoders.pkl', 'rb'))

# Prepare new data
new_order = {
    'Order_Date': '2024-12-01',
    'Region': 'North',
    'Country': 'USA',
    'Sales_Rep': 'John Smith',
    'Team': 'Team A',
    'Customer_Segment': 'Corporate',
    'Product_Category': 'Electronics',
    'Product_Name': 'Laptop Pro',
    'Stage': 'Opportunity',
    'Units_Sold': 10,
    'Target': 5000,
    'Deal_Size': 5200
}

# Convert to DataFrame
df_new = pd.DataFrame([new_order])

# Feature engineering
df_new['Order_Date'] = pd.to_datetime(df_new['Order_Date'])
df_new['Year'] = df_new['Order_Date'].dt.year
df_new['Month'] = df_new['Order_Date'].dt.month
df_new['Quarter'] = df_new['Order_Date'].dt.quarter
df_new['Day_of_Week'] = df_new['Order_Date'].dt.dayofweek

# Encode categorical features
for col, encoder in label_encoders.items():
    if col in df_new.columns:
        df_new[col] = encoder.transform(df_new[col])

# Drop unnecessary columns
df_new = df_new.drop(['Order_Date'], axis=1)

# Scale features
df_scaled = scaler.transform(df_new)

# Make prediction
predicted_revenue = model.predict(df_scaled)[0]

print(f"Predicted Revenue: ${predicted_revenue:,.2f}")
```

### REST API Example

```python
from flask import Flask, request, jsonify
import pickle
import pandas as pd

app = Flask(__name__)

# Load models at startup
model = pickle.load(open('sales_model.pkl', 'rb'))
scaler = pickle.load(open('scaler.pkl', 'rb'))
label_encoders = pickle.load(open('label_encoders.pkl', 'rb'))

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        df = pd.DataFrame([data])
        
        # Preprocess
        df['Order_Date'] = pd.to_datetime(df['Order_Date'])
        df['Year'] = df['Order_Date'].dt.year
        df['Month'] = df['Order_Date'].dt.month
        df['Quarter'] = df['Order_Date'].dt.quarter
        df['Day_of_Week'] = df['Order_Date'].dt.dayofweek
        
        for col, encoder in label_encoders.items():
            if col in df.columns:
                df[col] = encoder.transform(df[col])
        
        df = df.drop(['Order_Date'], axis=1)
        df_scaled = scaler.transform(df)
        
        # Predict
        prediction = model.predict(df_scaled)[0]
        
        return jsonify({
            'predicted_revenue': float(prediction),
            'status': 'success'
        })
    
    except Exception as e:
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 400

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### Batch Predictions

```python
# Load model and preprocessors
model = pickle.load(open('sales_model.pkl', 'rb'))
scaler = pickle.load(open('scaler.pkl', 'rb'))
label_encoders = pickle.load(open('label_encoders.pkl', 'rb'))

# Load batch data
batch_df = pd.read_csv('orders_to_predict.csv')

# Feature engineering
batch_df['Order_Date'] = pd.to_datetime(batch_df['Order_Date'])
batch_df['Year'] = batch_df['Order_Date'].dt.year
batch_df['Month'] = batch_df['Order_Date'].dt.month
batch_df['Quarter'] = batch_df['Order_Date'].dt.quarter
batch_df['Day_of_Week'] = batch_df['Order_Date'].dt.dayofweek

# Encode categorical features
for col, encoder in label_encoders.items():
    if col in batch_df.columns:
        batch_df[col] = encoder.transform(batch_df[col])

# Keep Order_ID for output
order_ids = batch_df['Order_ID'] if 'Order_ID' in batch_df.columns else range(len(batch_df))

# Drop unnecessary columns
columns_to_drop = ['Order_ID', 'Order_Date', 'Revenue'] if 'Revenue' in batch_df.columns else ['Order_ID', 'Order_Date']
batch_df = batch_df.drop(columns_to_drop, axis=1)

# Scale and predict
batch_scaled = scaler.transform(batch_df)
predictions = model.predict(batch_scaled)

# Create results DataFrame
results_df = pd.DataFrame({
    'Order_ID': order_ids,
    'Predicted_Revenue': predictions
})

results_df.to_csv('revenue_predictions.csv', index=False)
print(f"Predictions saved for {len(results_df)} orders")
```

## Monitoring & Maintenance

### Key Metrics to Track

1. **Model Performance**
   - RMSE threshold: Alert if > baseline RMSE Ã— 1.2
   - RÂ² threshold: Alert if < baseline RÂ² Ã— 0.9
   - Prediction latency: Target < 100ms

2. **Data Quality**
   - Missing value rate
   - Feature distribution shifts
   - New categorical values
   - Outlier frequency

3. **Business Metrics**
   - Forecast vs actual comparison
   - Inventory optimization impact
   - Revenue achievement rates
   - Stockout reduction

### Retraining Schedule

- **Monthly**: Regular retraining with new data
- **Quarterly**: Full model reevaluation and hyperparameter tuning
- **Emergency**: If performance drops >15%
- **A/B Testing**: Compare new models vs current production model

### Data Drift Detection

```python
from scipy import stats

def detect_drift(training_data, production_data, threshold=0.05):
    """Detect statistical drift in features"""
    drift_features = []
    
    for col in training_data.columns:
        # Kolmogorov-Smirnov test
        statistic, p_value = stats.ks_2samp(
            training_data[col], 
            production_data[col]
        )
        
        if p_value < threshold:
            drift_features.append(col)
            print(f"Drift detected in {col}: p-value = {p_value:.4f}")
    
    return drift_features
```

### Performance Monitoring

```python
def monitor_predictions(y_true, y_pred, baseline_rmse):
    """Monitor prediction quality"""
    current_rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    rmse_change = ((current_rmse - baseline_rmse) / baseline_rmse) * 100
    
    if rmse_change > 20:
        print(f"âš ï¸ ALERT: RMSE increased by {rmse_change:.1f}%")
        return True
    else:
        print(f"âœ“ Performance stable: RMSE change = {rmse_change:.1f}%")
        return False
```

## Best Practices

### Data Quality
- âœ… Validate input data before predictions
- âœ… Handle missing values appropriately
- âœ… Check for outliers and anomalies
- âœ… Ensure categorical values are in training set
- âœ… Maintain consistent date formats

### Model Training
- âœ… Use time-based splits for temporal data
- âœ… Perform cross-validation for robustness
- âœ… Save all preprocessing artifacts
- âœ… Document feature engineering steps
- âœ… Version control models and code

### Deployment
- âœ… Implement input validation
- âœ… Add logging for all predictions
- âœ… Set up automated monitoring
- âœ… Have rollback plan ready
- âœ… Test thoroughly before production

### Business Integration
- âœ… Provide confidence intervals with predictions
- âœ… Explain model decisions to stakeholders
- âœ… Set realistic expectations
- âœ… Use predictions as decision support, not sole decision maker
- âœ… Gather feedback for continuous improvement

## Troubleshooting

### Common Issues

**Issue**: Poor model performance on new data
```python
# Solution: Check for data drift and retrain
drift_features = detect_drift(X_train, X_new)
if len(drift_features) > 3:
    print("Significant drift detected. Retraining recommended.")
```

**Issue**: Predictions are consistently off
```python
# Solution: Check for systematic bias
bias = (y_pred - y_true).mean()
if abs(bias) > threshold:
    print(f"Systematic bias detected: {bias}")
```

**Issue**: New categorical values
```python
# Solution: Handle unknown categories
for col in categorical_features:
    if col in df.columns:
        # Add "Unknown" class handling
        unknown_mask = ~df[col].isin(label_encoders[col].classes_)
        if unknown_mask.any():
            df.loc[unknown_mask, col] = 'Unknown'
```

**Issue**: Memory errors with large datasets
```python
# Solution: Use batch processing
batch_size = 1000
predictions = []
for i in range(0, len(df), batch_size):
    batch = df[i:i+batch_size]
    pred = model.predict(batch)
    predictions.extend(pred)
```

## Project Structure

```
sales-prediction/
â”‚
â”œâ”€â”€ sales_prediction.ipynb          # Main notebook
â”œâ”€â”€ supermarket_sales.csv           # Input data
â”œâ”€â”€ sales_model.pkl                 # Trained model
â”œâ”€â”€ scaler.pkl                      # Feature scaler
â”œâ”€â”€ label_encoders.pkl              # Categorical encoders
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ api.py                      # Flask API
â”‚   â”œâ”€â”€ batch_predict.py            # Batch scoring
â”‚   â””â”€â”€ monitoring.py               # Performance tracking
â””â”€â”€ notebooks/
    â”œâ”€â”€ eda.ipynb                   # Exploratory analysis
    â””â”€â”€ model_tuning.ipynb          # Hyperparameter optimization
```

## Future Enhancements

1. **Deep Learning**: LSTM/GRU for time-series patterns
2. **Ensemble Methods**: Stack multiple models
3. **AutoML**: Automated hyperparameter tuning
4. **Feature Selection**: Recursive feature elimination
5. **Explainability**: SHAP values for interpretability
6. **Real-time Predictions**: Streaming data integration
7. **Multi-step Forecasting**: Predict multiple periods ahead
8. **Probabilistic Forecasting**: Quantile regression for uncertainty

## References

### Documentation
- [scikit-learn](https://scikit-learn.org/)
- [XGBoost](https://xgboost.readthedocs.io/)
- [pandas](https://pandas.pydata.org/)

### Research Papers
- XGBoost: Chen & Guestrin (2016)
- Random Forests: Breiman (2001)

## License

This project is provided as-is for educational and commercial use.

## Support

For questions or issues:
1. Check the troubleshooting section
2. Verify data format matches requirements
3. Ensure all dependencies are installed
4. Review execution logs for error messages

---

**Version**: 1.0  
**Last Updated**: 2025  
**Python**: 3.7+  
**Status**: Production Ready  
**Best Model**: XGBoost (RÂ² > 0.95)
